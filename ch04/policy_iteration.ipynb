{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Implementing policy evaluation and policy improvement for a deterministic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outside(i):\n",
    "    \"\"\"Check if the we are still in the grid\n",
    "    \n",
    "    Arguments:\n",
    "        i {int} -- Index of the row or column\n",
    "    \n",
    "    Returns:\n",
    "        is_out -- True if the index takes us outside the grid, else False\n",
    "    \"\"\"    \n",
    "    if (i < 0) or (i >= GRID_SIZE):\n",
    "        return True\n",
    "\n",
    "def is_terminal(state):\n",
    "    \"\"\"Check if the state is a terminal state\n",
    "    \n",
    "    Arguments:\n",
    "        state {tuple} -- A tuple (x, y) containing the position in the grid\n",
    "    \n",
    "    Returns:\n",
    "        bool -- True if state is terminal, else False\n",
    "    \"\"\"    \n",
    "    x, y = state\n",
    "    return (x == 0 and y == 0) or (x==GRID_SIZE-1 and y == GRID_SIZE-1)\n",
    "\n",
    "def take_action(state, action):\n",
    "    \"\"\"Take an action from a given state\n",
    "    \n",
    "    Arguments:\n",
    "        state {tuple} -- A tuple (x, y) containing the position in the grid\n",
    "        action {str} -- An action from a possible set of actions\n",
    "    \n",
    "    Returns:\n",
    "        (state, reward) -- A tuple (x, y) denoting the new state and a reward \n",
    "    \"\"\"    \n",
    "    x, y = state\n",
    "    if is_terminal(state):\n",
    "        return (x, y), 0\n",
    "    dy, dx = ACTIONS[action]\n",
    "    x_new, y_new = x+dx, y+dy\n",
    "    if not is_outside(x_new):\n",
    "        x = x_new\n",
    "    if not is_outside(y_new):\n",
    "        y = y_new\n",
    "    return (x, y), REWARD\n",
    "\n",
    "def draw_grid(grid):\n",
    "    \"\"\"Draw a table from the grid\n",
    "    \n",
    "    Arguments:\n",
    "        grid {np.ndarray} -- A Grid to be displayed\n",
    "    \n",
    "    Returns:\n",
    "        fig -- The figure containing the table\n",
    "    \"\"\"    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "\n",
    "    nrows, ncols = grid.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(grid):\n",
    "        tb.add_cell(i, j, width, height, text=val,\n",
    "                    loc='center', facecolor='white')\n",
    "\n",
    "        # Row and column labels...\n",
    "    for i in range(len(grid)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n",
    "                    edgecolor='none', facecolor='none')\n",
    "    ax.add_table(tb)\n",
    "    return fig\n",
    "\n",
    "def init_policy(GRID_SIZE):\n",
    "    \"\"\"Generates a random deterministic policy\n",
    "    \n",
    "    Arguments:\n",
    "        GRID_SIZE {int} -- Size of the NxN grid world\n",
    "    \n",
    "    Returns:\n",
    "        policy -- A random deterministic policy\n",
    "    \"\"\"    \n",
    "    policy = np.random.choice(['←', '→', '↑', '↓'], size=(GRID_SIZE, GRID_SIZE))\n",
    "    policy[0, 0] = 'x'\n",
    "    policy[GRID_SIZE-1, GRID_SIZE-1] = 'x'\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the parameters for the Grid and initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 10\n",
    "GRID = np.zeros((GRID_SIZE, GRID_SIZE)) \n",
    "ACTIONS = {\n",
    "    \"←\": [-1, 0],\n",
    "    \"↑\": [0, -1],\n",
    "    \"→\": [1, 0],\n",
    "    \"↓\": [0, 1]\n",
    "}\n",
    "POLICY = init_policy(GRID_SIZE)\n",
    "REWARD = -1\n",
    "GAMMA = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the initial random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_grid(POLICY)\n",
    "fig.savefig('./images/before_iteration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "Given a policy $\\pi$, evaluate how good the policy is. That is, compute the value function for the given policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(grid, policy, gamma=1.0):\n",
    "    \"\"\"Evaluate the given policy for the grid\n",
    "    \n",
    "    Arguments:\n",
    "        grid {np.ndarray} -- The grid containing the value of the states\n",
    "        policy {np.ndarray} -- The deterministic policy which shows actions to take in each state\n",
    "        gamma {float} -- The discounting factor\n",
    "    \"\"\"    \n",
    "    iter_count = 0\n",
    "    while True:\n",
    "        iter_count += 1\n",
    "        old_value = grid.copy()\n",
    "        delta = 0\n",
    "        for i in range(grid.shape[0]):\n",
    "            for j in range(grid.shape[0]):\n",
    "                current_state = (i, j)\n",
    "                action = policy[current_state]\n",
    "\n",
    "                new_state, reward = take_action(current_state, action)\n",
    "                value = reward + gamma*GRID[new_state]\n",
    "\n",
    "                grid[current_state] = value\n",
    "                delta = max(delta, np.abs(old_value[current_state] - grid[current_state]))\n",
    "        if delta < 1e-6 or iter_count == 1000:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement\n",
    "Given a policy $\\pi$, improve the policy using the Policy Improvement Theorem.\n",
    "\n",
    "For each state $s$, take actions $a$ and then follow $pi$ afterwards.\n",
    "\n",
    "Take the action which gives the maximum value from the above computation.\n",
    "\n",
    "If the new maximum value is greater than $v_{\\pi}(s)$, update the policy $\\pi(s)$ with the action that gave the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(grid, policy, actions, gamma):\n",
    "    \"\"\"Improve the policy using policy improvement theorem\n",
    "    \n",
    "    Arguments:\n",
    "        grid {np.ndarray} -- The grid containing the value of the states\n",
    "        policy {np.ndarray} -- The deterministic policy which shows actions to take in each state\n",
    "        actions {dict} -- A dictionary showing how to change the state based on the action\n",
    "        gamma {float} -- The discounting factor\n",
    "    \n",
    "    Returns:\n",
    "        bool -- True if the policy cannot be further improved else False\n",
    "    \"\"\"    \n",
    "    policy_stable = True\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[0]):\n",
    "            current_state = (i, j)\n",
    "            current_value = grid[current_state]\n",
    "            old_action = policy[current_state]\n",
    "            values = []\n",
    "            for action in actions:\n",
    "                new_state, reward = take_action(current_state, action)\n",
    "                value = reward + gamma*grid[new_state]\n",
    "                values.append([action, value])\n",
    "            max_action, max_value = max(values, key=itemgetter(1))\n",
    "            if max_value > current_value:\n",
    "                policy[current_state] = max_action\n",
    "            if old_action != policy[current_state]:\n",
    "                policy_stable = False\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Policy Iteration\n",
    "\n",
    "Evaluate -> Improve -> Evaluate -> Improve ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "for i in range(max_iter):\n",
    "    policy_evaluation(GRID, POLICY, gamma=GAMMA)\n",
    "    converged = policy_improvement(GRID, POLICY, ACTIONS, gamma=GAMMA)\n",
    "    if converged:\n",
    "        print(f'Stable Policy Found! Took {i+1} iterations.')\n",
    "        break\n",
    "else:\n",
    "    print('Maximum iterations exceeded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_grid(POLICY)\n",
    "fig.savefig('images/after_iteration.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
